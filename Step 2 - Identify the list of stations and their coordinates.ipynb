{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Recover train network from train schedules\n",
    "\n",
    "In this notebook we recover the graph of the train network of Belgium from the data of railway schedules, geographical railway track data and stations GPS coordinates. This relies on the positional accuracy of datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import geopandas as gp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, time\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(month_start, year_start, month_end, year_end):\n",
    "    month_range = list(range(1,13))\n",
    "    cycle_month_range = cycle(month_range)\n",
    "    while True:\n",
    "        current_month = next(cycle_month_range)\n",
    "        if current_month == month_start:\n",
    "            break\n",
    "    date_tuples = []\n",
    "    year = year_start\n",
    "    while True:\n",
    "        if current_month < 10:\n",
    "            date_tuples.append((\"0\"+str(current_month), str(year)))\n",
    "        else:\n",
    "            date_tuples.append((str(current_month), str(year)))\n",
    "        if year == year_end and current_month == month_end:\n",
    "            break\n",
    "        current_month = next(cycle_month_range)\n",
    "        if current_month == 1:\n",
    "            year += 1\n",
    "    return date_tuples\n",
    "\n",
    "\n",
    "def upload_schedule(in_file):\n",
    "    def date_hook(json_dict):\n",
    "        for (key, value) in json_dict.items():\n",
    "            for index_list_entry in range(len(value)):\n",
    "                for index_entry, v in enumerate(value[index_list_entry]):\n",
    "                    try:\n",
    "                        json_dict[key][index_list_entry][index_entry] = datetime.strptime(\n",
    "                            v, \"%Y-%m-%d %H:%M:%S\"\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "        return json_dict\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        return json.load(in_f, object_hook=date_hook) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passenger trains schedules\n",
    "\n",
    "We use this data to decide whether two stations can be linked together. Provide your custom start dates to have coarser or more precise result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-09-01', '2019-09-02', '2019-09-03', '2019-09-04', '2019-09-05', '2019-09-06', '2019-09-07', '2019-09-08', '2019-09-09', '2019-09-10', '2019-09-11', '2019-09-12', '2019-09-13', '2019-09-14', '2019-09-15', '2019-09-16', '2019-09-17', '2019-09-18', '2019-09-19', '2019-09-20', '2019-09-21', '2019-09-22', '2019-09-23', '2019-09-24', '2019-09-25', '2019-09-26', '2019-09-27', '2019-09-28', '2019-09-29', '2019-09-30']\n"
     ]
    }
   ],
   "source": [
    "start_month = 9\n",
    "start_year = 2019\n",
    "end_month = 9\n",
    "end_year = 2019\n",
    "\n",
    "total_path_list = []\n",
    "total_files_list = []\n",
    "\n",
    "date_range = get_date_range(start_month, start_year, end_month, end_year)\n",
    "base_in_dir = Path(\"./infrabel_data_json/\")\n",
    "for month, year in date_range:\n",
    "    data_dir = os.path.join(base_in_dir, \"-\".join([year, month]))\n",
    "    files_list = sorted(os.listdir(data_dir))\n",
    "    path_list = [os.path.join(data_dir, f) for f in files_list]\n",
    "    total_path_list += path_list\n",
    "    total_files_list += files_list\n",
    "print(total_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1384e2061cf544d7844f67dd88c9e13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schedule_loaded = []\n",
    "for day_file, upload_path in tqdm(list(zip(total_files_list, total_path_list))):\n",
    "    schedule_loaded.append((day_file, upload_schedule(upload_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recover the network from schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project all intra-country train track records onto the set of stations\n",
    "\n",
    "The schedules contain records of train passage through stations on its way in a form of a consecutive list. We imply that two consecutive stations can be reached and put a link between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac5c476e49c4e44a0555305d8d98cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 669 nodes and 1164 edges\n"
     ]
    }
   ],
   "source": [
    "H = nx.Graph()\n",
    "\n",
    "for day, day_schedule in tqdm(schedule_loaded):\n",
    "    for train, schedule in day_schedule.items():\n",
    "\n",
    "        first_time = True\n",
    "        for row1, row2 in zip(schedule, schedule[1:]):\n",
    "            previous_station = row1[0]\n",
    "            current_station = row2[0]\n",
    "            \n",
    "            if first_time:\n",
    "                previous_id = row1[-3]\n",
    "                line_dep = row1[-1]\n",
    "                if not H.has_node(previous_station):\n",
    "                    H.add_node(previous_station, count = 1)\n",
    "                first_time = False\n",
    "            current_id = row2[-3]\n",
    "            if not H.has_node(current_station):\n",
    "                H.add_node(current_station, count = 1)\n",
    "            else:\n",
    "                H.nodes[current_station][\"count\"] += 1\n",
    "            \n",
    "            line_dep = row1[-1]\n",
    "            line_arr = row2[-2]\n",
    "            if not H.has_edge(previous_station, current_station):\n",
    "                H.add_edge(previous_station, current_station, count = 1)\n",
    "            else:\n",
    "                H[previous_station][current_station][\"count\"] += 1\n",
    "\n",
    "for e1,e2 in H.edges():\n",
    "    H[e1][e2][\"count\"] /= len(schedule_loaded)\n",
    "for u in H.nodes():\n",
    "    H.nodes[u][\"count\"] /= len(schedule_loaded)\n",
    "    \n",
    "print(nx.info(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_counts = sorted([(dd[\"count\"], u) for u,dd in H.nodes(data = True)], key = lambda x: x[0])\n",
    "edge_counts = sorted([(dd[\"count\"], e1,e2) for e1,e2,dd in H.edges(data = True)], key = lambda x: x[0])\n",
    "\n",
    "nodes_small_number_trains = [u for d, u in node_counts if d < 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and remove all garbage stations\n",
    "\n",
    "We will delete all stations which are not passenger train stations, such as garages, wash stations, etc. They have appropriate suffixes. Also there is a shunting yard at SCHAARBEEK, which we delete as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0d6a1f69db426d88a75a3b6885225c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26976eb479dc4e49bff19856d5930186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_suffix_stations = []\n",
    "suffixes = [\"WIJKSPOOR\", \"SAS\", \"GARAGE\", \"CARWASH\", \"AFWISSELING\", \"FORMATION\", \n",
    "            \"GOEDEREN\", \"BUNDEL\", \"BUNDELS\", \"FAISCEAU\", \"MARCHANDISES\",\"WIJKBUNDEL\", \n",
    "            \"A.T.\", \"T.W.\", \"RELAIS\", \"PERRON\", \"WIJKSBUNDEL\"]\n",
    "for u in tqdm(H.nodes()):\n",
    "    if isinstance(u, int):\n",
    "        continue\n",
    "    for v in H.nodes():\n",
    "        if isinstance(v, int):\n",
    "            continue\n",
    "        trim_v = v.split(\" \")[0]\n",
    "        split_v = trim_v.split(\"-\")\n",
    "        if len(split_v) > 1:\n",
    "            ending = split_v.pop(-1)\n",
    "            beginning = \"-\".join(split_v)\n",
    "            if beginning == u:\n",
    "                if ending in suffixes:\n",
    "                    bad_suffix_stations.append(v)\n",
    "# add schaarbeek constellation\n",
    "possible_schaarbeeks = [\"SCHAARBEEK\", \"SCHAARBEEK-JOSAPHAT\"]\n",
    "for v in tqdm(H.nodes()):\n",
    "    if isinstance(v, int):\n",
    "        continue\n",
    "    if \"SCHAARBEEK\" in v:\n",
    "        if v not in possible_schaarbeeks and v not in bad_suffix_stations:\n",
    "            bad_suffix_stations.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the non garbage stations \n",
    "\n",
    "We remove the garbage stations from the list of all nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_stations = set(H.nodes()) - set(bad_suffix_stations) - set(nodes_small_number_trains)\n",
    "len(remaining_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store geo positions of remaining stations\n",
    "\n",
    "Supplementary data is downloaded from here: https://infrabel.opendatasoft.com/explore/dataset/operationele-punten-van-het-newterk/information/. This is a file with all operational points in the Belgian railway network ans their coordinates. We select only stations (fr. \"Point d'arrêt\", \"Gare\") and store their coordinates ss attributes on the network nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03833f01cefb4198b914e0c57a4bb6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stops_file = \"./supplementary_data/points-operationnels-du-reseau.csv\"\n",
    "stat_pd = pd.read_csv(stops_file, header = 0, sep = \";\")\n",
    "\n",
    "coordinates_dict = {}\n",
    "\n",
    "stat_stations = stat_pd[stat_pd[\"Classification\"].isin([\"Point d'arrêt\", \"Gare\"])]\n",
    "\n",
    "for index, row in tqdm(stat_stations.iterrows(), total = len(stat_stations)):\n",
    "    station_name = row[\"Abréviation LST NL complète\"]\n",
    "    if (station_name not in coordinates_dict):\n",
    "        coords = row[\"Geo Point\"].split(\",\")\n",
    "        X,Y = float(coords[1]), float(coords[0])\n",
    "        coordinates_dict[row[\"Abréviation LST NL complète\"]] = (X,Y)\n",
    "        \n",
    "# station_names, station_points = zip(*list(coordinates_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recover the position of missing stations\n",
    "\n",
    "The dataset turns out to be incomplete, therefore we recover the missing coordinates using the spring layout. We fix the known coordinates apriori. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pos = {}\n",
    "fixed_nodes = []\n",
    "for u in H.nodes():\n",
    "    if u in coordinates_dict:\n",
    "        node_pos[u] = coordinates_dict[u]\n",
    "        fixed_nodes.append(u)\n",
    "    else:\n",
    "        node_pos[u] = (4,50)\n",
    "        \n",
    "pos = nx.spring_layout(H, pos = node_pos, fixed = fixed_nodes, iterations = 50, k = 1/10000)\n",
    "\n",
    "for u in H.nodes():\n",
    "    H.nodes[u][\"lon\"] = pos[u][0]\n",
    "    H.nodes[u][\"lat\"] = pos[u][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_positions = {}\n",
    "for u in remaining_stations:\n",
    "    if u in coordinates_dict:\n",
    "        node_positions[u] = coordinates_dict[u]\n",
    "    else:\n",
    "        node_positions[u] = list(pos[u])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dict with station coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_data_dir = \"./intermediate_data/\"\n",
    "os.makedirs(intermediate_data_dir, exist_ok=True)\n",
    "nodes_with_coordinates_filepath = os.path.join(\n",
    "    intermediate_data_dir, \"remaining_nodes_with_coordinates.pkl\"\n",
    ")\n",
    "pickle.dump(node_positions, open(nodes_with_coordinates_filepath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
